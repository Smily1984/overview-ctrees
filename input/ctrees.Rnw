``Classic" partitioning algorithms often have a kind of loss function, which is to be minimized with the next split. The optimization is done by an exhaustive search over all possible split points. After splitting the data, the steps are repeated for the new partitions until a stop criterion dictates to stop.  \\ 

Recursive partitioning with the conditional trees algorithm works different and can be summarized as follows: \\
In contrast to CART or other algorithms, variable selection and the search for the best split are strictly seperated. Dependency between response and covariate is tested with hypothesis tests. To get rid of the problem of different scales for different covariates, the measurement for the association is the p-value. Only then, after choosing, the covariate is searched for the best split point. The stopping criterion is formulated in terms of statistical test theory as well: stop when the null-hypothesis of independence cannot be rejected any more. \\

All of the hypothesis testing is done by permutation tests. For a quick introduction to permutation tests, please visit the appendix. LINK. A good idea of permutation tests is required to understand the recursive partitioning algorithm.  \\

When using permutation tests you are free to choose the test statistic. Some are better than others. It will depend on the scales of the response and the covariate and on how you want to formulate the problem. The algorithm uses test statistics suggested by a framework developed by Strasser and Weber QUOTE. They offer a very general formulatin of a test statistic for permutation tests, which can handle arbitrary scales for response and covariate. AND THEY HAVE NICE ASYMPTOTICS?? The formula is presented in detail later. \\

The following chapter explains the algorithm in detail. 
