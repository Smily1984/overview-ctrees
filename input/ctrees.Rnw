``Classic" recursive partitioning algorithms often have a kind of loss function, which is to be minimized with the next split. The optimization is done by an exhaustive search over all possible split points. After splitting the data, the steps are repeated for the new partitions until a stop criterion dictates to stop.  \\ 

Recursive partitioning with the conditional trees algorithm works different. \marginnote{Summary of the conditional trees algorithm}
In contrast to CART or other algorithms, variable selection and the search for the best split are strictly seperated. Dependency between response and covariate is tested with hypothesis tests. To get rid of the problem of different scales for different covariates, the measurement for the association is the p-value. After choosing a covariate for the split, it is searched for the best split point. The stopping criterion is formulated in terms of statistical test theory as well: stop when the null-hypothesis of independence cannot be rejected any more. \\

All of the hypothesis testing is done by permutation tests. \sidenote{For a quick introduction to permutation tests, please visit the appendix, page \pageref{chap:perm}. A good idea of permutation tests is required to understand the recursive partitioning algorithm.}
When using permutation tests you are free to choose the test statistic. Some are better than others. It will depend on the scales of the response and the covariate and on how you want to formulate the problem. The test statistics used by the algorithm is derived from a framework developed by \citet{strasser}. They offer a very general formulation of a test statistic for permutation tests, which can handle arbitrary scales for response and covariate. \\

The following chapter explains the algorithm in detail. 
