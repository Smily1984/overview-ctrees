All decisions of the algorithm are embeded in hypothesis tests. These are done with permutation tests (conditional inference).  \\
Strasser and Weber [LINK] have formulated a very general test statistic, which can be used to do a permutation 
test if a response $Y$ and a covariate $X$ are independent.  
\[\mathbf{T}_j(L_n, w) = vec \left( \sum\limits_{i = 1}^{n} w_i g_j (X_{ij}) h(Y_i, (Y_1, ..., Y_n))^T \right) \in \mathbb{R}^{p_j q}\] \\
This test statistic might look difficult in the first place, because it is a very general formulation. When looking at particular examples the formula becomes very friendly and seems more natural. This can be seen in the two included examples for regression and classification LINK. 
At this point you can accept the test statistic as it is or you can read more in the appendix LINK about it.  \\
This test statistic is standardized before it is used. 
 \(c(\mathbf{t}, \mu, \Sigma) = \max_{k = 1, \dots, pq} \left| \frac{\left({\mathbf{t} - 
  \mathbf{\mu}}\right)_k}{\sqrt{(\Sigma)_{kk}}}\right|\)  
 
Formulas for mean and variance of $T$ is also in the appendix LINK. 
Thus $c$ is the test statistic which is calculated for each permutation and extreme values for c for the observed data compared to the permutations will lead to rejecting the null hypothesis of independence. 
 
