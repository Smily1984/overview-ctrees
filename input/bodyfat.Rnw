The first example is a continuous regression model, where both the response and the covariates
are measured on a numeric scale.\marginnote{$Y$, $X_j$ numeric, $ \quad j = 1, \dots, m$ } The model is illustrated with the \textit{bodyfat} data set, which is available in
the mboost package developed by \citet{mboost}. 
\subsection*{Data set}
The data set contains observations of 71 healthy women.\marginnote{$n = 71$} The data contains body fat values, which 
are measured by DXA (Dual-energy X-ray absorptiometry), a method to determine the amount of body fat. 
Other variables in the data set are anthropometric measurements like the breadth of the knee, the waist circumference, the hip circumference etc.. 
The objective is to predict the body fat\marginnote{Predict bodyfat with body measurements as input} with the anthropometric measurements, because the DXA method is more expensive and 
not always available.  
\subsection*{Test statistic}
Bodyfat measured by DXA as well as body measurements are numeric variables. Thus one possible choice for the
influence function $h$ and the transformation function $g_j, \quad j = 1, \dots, m$ is the identity function, which means
the variables will not be transformed at all.  \\
Thus: 
\[h = \mathbf{Y}_i \quad \text{and} \quad g_j = \mathbf{X}_j \quad j = 1, \dots, m\]
This yields following not-standardized test statistic:\sidenote{The formulation $\sum\nolimits_{i \in node}$ means sum over all observations in the node (= partition). This is the same as the sum over all observations with additional weights, because only observations in the current node have weight $w = 1$, the others have weights $w = 0$} 

\[\mathbf{T}_{j}(\mathcal{L}_n, \mathbf{w}) = \sum\limits_{i=1}^n w_i \mathbf{X}_{ji} \mathbf{Y}_i  = \sum\limits_{i \in node} \mathbf{X}_{ji} \mathbf{Y}_i\]
The next step is to standardize the test statistic:\sidenote{Definitions of $\mu$ and $\Sigma$ can be found on page \pageref{chap:strasser}}
\sidenote{\(n_{node} := \sum\limits_{i=1}^n w_i = \)Number of observation in node  \\
\(\bar{Y}_{node}\): Mean of $Y$ in node \\
\(\bar{X}_{j, node}\): Mean of $X_{j}$ in node}
\[c_{max}(\mathbf{t}, \mu, \Sigma)  =  \max_{k=1, \dots, pq}\left|\frac{(t - \mu)_k}{\sqrt{(\Sigma)_{kk}}}\right|  = 
\left|\frac{t-\mu}{\sqrt{\Sigma}}\right| \]
\begin{align*}
  \mu_j & = \left(\sum\limits_{i = 1}^n w_{i} X_{ji} \right) \mathbb{E}_{node}(h) = n_{node} \cdot \bar{X}_{j, node} \bar{Y}_{node}\\ 
 \Sigma & = \frac{n_{node}}{n_{node} - 1} \mathbb{V}_{node}(h) \cdot \sum\limits_{i=1}^n w_i X_{ji}^2 - \frac{1}{n_{node} - 1} \mathbb{V}_{node}(h) n_{node}^2 \bar{X}_{j, node}^2 \\
        & = \frac{1}{n_{node}} \sum\limits_{i \in node} (Y_i - \bar{Y}_{node})^2 \sum\limits_{i=1}^n X_{ji}^2 \\
 & - \frac{1}{n_{node} - 1} \cdot \frac{1}{n_{node}} \sum\limits_{i \in node} (Y_i - \bar{Y}_{node})^2 n_{node}^2 \bar{X}_{j, node}^2 \\
        & = \frac{1}{n_{node} - 1} \sum\limits_{i \in node}\left(Y_i - \bar{Y}_{node}\right)^2 \left(\sum\limits_{i \in node} X_{ji}^2 - n_{node} \bar{X}_{j, node}^2\right)  \\
        & = \frac{1}{n_{node} - 1} \left(\sum\limits_{i \in node} (Y_i - \bar{Y}_{node})^2\right) \left( \sum\limits_{i \in node} (X_{ji} - \bar{X}_{j, node})^2\right)   
\end{align*}
Therefore: 
\[c
\propto 
\left| \frac{\sum\limits_{i \in node} X_{ji} Y_i - n_{node} \bar{X}_{j, node} \bar{Y}_{node}}{\sqrt{
    \left(\sum\limits_{i \in node}(Y_i - \bar{Y}_{node})^2\right)\left(\sum\limits_{i \in node} (X_{ji} - \bar{X}_{j, node})^2\right)}} \right|\]
  

The linear test statistic is proportional to Pearson's correlation coefficient. This means, that the permutation test is testing
if the correlation between $Y$ and any $X_j$ is different than zero. Thus by choosing the identity function for $h$ and $g_j$ the null 
hypothesis of independence between $Y$ and $X_j$ is formulated as ``The correlation between $Y$ and $X_j$ is zero''.  \\ 
The next step is to calculate the test statistic (the Pearson's correlation coefficient multiplied with a constant) for the 
observation in the current partition (where $w \neq 0$). 
The response of the observations will be permuted and the test statistic calculated again. This will be done often enough to 
approximate the distribution of the test statistic for the sample. If the correlation coefficient of the original data is 
very extreme compared to the permuted test statistics, the p-value will be very low. \\
The procedure of calculating the test statistic for the original data and the permutations is done for every 
covariate $X_j, \quad j \in 1, \dots, m$ separately. \\ 


\subsection*{Test statistic for splitting criteria}
For regression and the identity function for the influence function $h$, the statistic used to find the best split is the following:
\sidenote{
\(n_A\): Number of observations in partition $A$ \\
\(\bar{Y}_A\): Mean of $Y$ in $A$  
}
\[\mathbf{T}_{j*}^A(\mathcal{L}_n, \mathbf{w}) = \sum\limits_{i = 1}^n w_i I(X_{j*i} \in A)\cdot Y_i  = \sum\limits_{i: x_{j*i} \in A} Y_i  = n_A \bar{Y}_A\]
\[\mu_{j*}^A = \sum\limits_{i=1}^n w_i I(X_{j*i} \in A) \cdot \frac{1}{n_{node}} \sum\limits_{i=1}^n w_i Y_i = n_A \bar{Y}_{node}\]
\begin{align*}
  \Sigma_{j*}^A &= \frac{n_{node}}{n_{node} - 1} \frac{1}{n_{node}} \sum\limits_{i=1}^n w_i (Y_i - \bar{Y}_{node})^2 \cdot \sum\limits_{i=1}^n w_i I(X_{j*i} \in A)^2 \\
  & - \frac{1}{n_{node} - 1}  \frac{1}{n_{node}}\sum\limits_{i=1}^n w_i (Y_i - \bar{Y}_{node})^2 \cdot \left( \sum\limits_{i=1}^n w_i I(X_{j*i} \in A)\right)^2\\
  & = \frac{1}{n_{node} -1} \sum\limits_{i \in node} (Y_i - \bar{Y}_{node})^2 n_A \left(1 - \frac{n_A}{n_{node}}\right) \\
    & = Var(Y_{node}) \cdot Var(Z)\\
\end{align*}

with
\[Z \sim B(n_{node}, \pi = \frac{n_A}{n_{node}})\]
Can be interpreted as the probability that z observations would be assigned to $A$ if the process of assigning would be random with probability $\frac{n_A}{n_{node}}$. 
%% Is maximal for \(n_A = \frac{n_{node}}{2}\). 
%% The closer \(\frac{n_A}{n_{node}}\) to 0.5 the bigger is $c$ (assuming $Y_A$ stays the same).
Thus the standardized test statistic is: 
\[ c_{max}(\mathbf{t}^A_{j*}, \mu^A_{j*}, \Sigma^A_{j*})  =  max_{k} \left| \frac{(\mathbf{t}^A-\mu)_k}{\sqrt{(\Sigma)_{kk}}}\right| 
   =  n_A \left| \frac{\bar{Y}_A - \bar{Y}_{node}}{\sqrt{Var(Y_{node}) \cdot Var(Z)}}\right|\] 
The partition which maximizes the above expression will be chosen. Maximizing $c_{max}$ means finding the partition, where the difference between the mean of $Y$ in the partition and the mean of $Y$ in the whole partition is large and the number of observations in $A$ is large at the same time. 




\subsection*{R-Code}
The data set can be loaded from mboost (\citet{mboost}) package using the \lstinline{data()} function: 
<<bodyfat-data>>=
data(bodyfat, package = "mboost")
@ 

The conditional tree algorithm is implemented in the party package (\citet{hothorn2006unbiased}), which is available on \url{http://cran.r-project.org/}.
The usage is similar to the \lstinline{lm()}-function, with the formula interface. 
The formula \lstinline{DEXfat ~ . } means that the tree should model the response DEXfat (bodyfat measurement) 
depending on all available covariates. 
<<bodyfat-plot-ctree, out.width = "12cm", out.extra = "trim = 3cm 0cm 0cm 1cm", fig.width = 14, fig.height = 8, cache = FALSE, fig.cap = "Conditional tree for bodyfat data", message = FALSE>>=
library("party")
## fit a conditional tree
cond_tree <- ctree(DEXfat ~ ., data = bodyfat)
plot(cond_tree)
@ 
The result is a tree (Figure \ref{fig:bodyfat-plot-ctree}) with six terminal nodes (five splits). The variable chosen for the first split is \lstinline{hipcirc}, the circumference of the hip in cm. If it is bigger than 108cm the next measurement to look at is the breadth of the knee (\lstinline{kneebreadth}). If the breadth is smaller than 10.6 cm the estimated is 39.7, which is equal to the mean in this terminal node. The interpretation for the other nodes is equivalent. 
<<bodyfat-rpart, out.extra  = "trim = 0cm 4cm 0cm 4cm", cache = FALSE, fig.width = 14, fig.height = 8, cache = FALSE, fig.cap = "CART tree for bodyfat data", out.width = "14cm">>=
library("rpart")
library("partykit")
cart <- rpart(DEXfat ~ ., data = bodyfat)
plot(as.party(cart))
@


The CART algorithm gives us a different tree (Figure \ref{fig:bodyfat-rpart}). In comparison, CART uses the circumference of the waist for the first split while the conditional trees algorithm uses the same measurement of the hip. There is even a covariate (kneebreadth) which is used by the conditional trees algorithm, but not used by the CART algorithm. The resulting trees are structurally different. 
