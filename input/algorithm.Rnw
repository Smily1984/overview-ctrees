In every partition, following steps are conducted\sidenote{Permutation test related steps are \color{red} marked in red \color{black}} 
See also the original formulation in \citet[Chapter 2 and 3]{hothorn2006unbiased}
\vspace{0.5cm}
\begin{fullwidth}
    \begin{enumerate}
    \item \textbf{Stop criterion} \\
      \begin{itemize}
      \item  Test global null hypothesis $H_0$ of independence between $Y$ and all $X_j$  
        with \\ $H_0 = \cap_{j=1}^m H_0^j$ and $H_0^j: D(\mathbf{Y}| X_j) = D(\mathbf{Y})$ \color{red} (permutation tests for each $X_j$) \color{black} \\
      \item If $H_0$ not rejected \color{red} (no significance for all \(X_j\)) \color{black} $\Rightarrow$ Stop
      \end{itemize}

    \item \textbf{Variable selection} \\
      Select covariate $X_{j*}$ with strongest association \color{red} (smallest p-value) \color{black}
    \item \textbf{Best split point search}  \\
      Search best split for $X_{j*}$ \color{red} (max. test statistic c) \color{black} and partition data
    \item \textbf{Repeat}  \\
      Repeat steps 1.), 2.) and 3.) for both of the new partitions
    \end{enumerate} 
\end{fullwidth} 
\vspace{0.5cm}
The algorithm starts with the whole data set and tests if it should be split. If the null hypothesis is rejected the variable with the strongest
association with the response is chosen and the partition is split into two new partitions. The steps will be repeated within both
of the new partitions.\sidenote{That's why it is called ``recursive'' partitioning''} Covariates chosen for a split can be chosen again later (only in the case of a binary covariate it doesn't work). 
The tree stops to grow, if in all partitions the global null hypothesis of independence cannot be rejected (stop criterion).
The next sections describe in detail how the single steps work and especially how permutation tests are applied. 
